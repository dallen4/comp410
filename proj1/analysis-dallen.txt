In this project, my goal was to calculate an estimate of the average overhead time for read/write system calls. Unsure if both the undergrad and grad dataset were to be supplied, both have been provided. Each provided a certain degree of insight into this concept.
There seems to be a direct correlation between the number of read/write operations necessary and the total time elapsed to perform said operation. Thus, with smaller buffer sizes, it is going to take longer because of the number of write operations need to complete the task.
By looking at the longest time that was produced by the smallest buffer size, and thus the largest write/read count, compared to the shortest time which was produced by the largest buffer size, and thus required the smallest write/read count, a hypothesis was able to be formed.
If you calculate the difference between these times over the difference in the read and/or write counts necessary to perform the given task, you can find an average time difference to read/write count different which could be an interesting metric. Albeit not the most accurate calculation to find the average overhead of a context switch between the user mode and the OS operations. It provides insight into the importance of buffer size when reading and writing to disk space.
To achieve fast read and write times, it is best to user higher buffer sizes (but reasonable) to reduce the number of context switches. This herein decreasing the total time it takes that process to complete its task.
